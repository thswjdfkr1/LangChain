{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8e061a-b2ac-434c-a46e-125c9c4c4737",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "375aa248-1888-4f77-8f70-c37b80baf5c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (0.2.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from langchain_community) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from langchain_community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from langchain_community) (0.6.6)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from langchain_community) (0.2.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from langchain_community) (0.2.5)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from langchain_community) (0.1.75)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from langchain_community) (2.32.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from langchain_community) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (0.2.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.7.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (2024.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.11.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.18.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ecee2-b7c4-43cc-a312-a4db951f2d3f",
   "metadata": {},
   "source": [
    "## 검색 할 파일 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f3a8d-fc46-4198-94f0-c55ed0804756",
   "metadata": {},
   "source": [
    "### txt 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e48e53-0a5b-48d4-9fe9-b0bfbe95b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./langchain/docs/docs/get_started/quickstart.mdx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e90a14c-f8cb-46b8-bf65-555229231b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56df2c70-905c-4e49-9019-52370dff083b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b781782e-5e68-49a6-9286-ed3403dc8b01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Quickstart\\n\\nIn this quickstart we\\'ll show you how to:\\n- Get setup with LangChain, LangSmith and LangServe\\n- Use the most basic and common components of LangChain: prompt templates, models, and output parsers\\n- Use LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chaining\\n- Build a simple application with LangChain\\n- Trace your application with LangSmith\\n- Serve your application with LangServe\\n\\nThat\\'s a fair amount to cover! Let\\'s dive in.\\n\\n## Setup\\n\\n### Jupyter Notebook\\n\\nThis guide (and most of the other guides in the documentation) use [Jupyter notebooks](https://jupyter.org/) and assume the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because often times things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\\n\\nYou do not NEED to go through the guide in a Jupyter Notebook, but it is recommended. See [here](https://jupyter.org/install) for instructions on how to install.\\n\\n### Installation\\n\\nTo install LangChain run:\\n\\nimport Tabs from \\'@theme/Tabs\\';\\nimport TabItem from \\'@theme/TabItem\\';\\nimport CodeBlock from \"@theme/CodeBlock\";\\n\\n<Tabs>\\n  <TabItem value=\"pip\" label=\"Pip\" default>\\n    <CodeBlock language=\"bash\">pip install langchain</CodeBlock>\\n  </TabItem>\\n  <TabItem value=\"conda\" label=\"Conda\">\\n    <CodeBlock language=\"bash\">conda install langchain -c conda-forge</CodeBlock>\\n  </TabItem>\\n</Tabs>\\n\\n\\nFor more details, see our [Installation guide](/docs/get_started/installation).\\n\\n### LangSmith\\n\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with [LangSmith](https://smith.langchain.com).\\n\\nNote that LangSmith is not needed, but it is helpful.\\nIf you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\\n\\n```shell\\nexport LANGCHAIN_TRACING_V2=\"true\"\\nexport LANGCHAIN_API_KEY=\"...\"\\n```\\n\\n## Building with LangChain\\n\\nLangChain enables building application that connect external sources of data and computation to LLMs.\\nIn this quickstart, we will walk through a few different ways of doing that.\\nWe will start with a simple LLM chain, which just relies on information in the prompt template to respond.\\nNext, we will build a retrieval chain, which fetches data from a separate database and passes that into the prompt template.\\nWe will then add in chat history, to create a conversation retrieval chain. This allows you interact in a chat manner with this LLM, so it remembers previous questions.\\nFinally, we will build an agent - which utilizes and LLM to determine whether or not it needs to fetch data to answer questions.\\nWe will cover these at a high level, but there are lot of details to all of these!\\nWe will link to relevant docs.\\n\\n## LLM Chain\\n\\nFor this getting started guide, we will provide two options: using OpenAI (a popular model available via API) or using a local open source model.\\n\\n<Tabs>\\n  <TabItem value=\"openai\" label=\"OpenAI\" default>\\n\\nFirst we\\'ll need to install their Python package:\\n\\n```shell\\npip install openai\\n```\\n\\nAccessing the API requires an API key, which you can get by creating an account and heading [here](https://platform.openai.com/account/api-keys). Once we have a key we\\'ll want to set it as an environment variable by running:\\n\\n```shell\\nexport OPENAI_API_KEY=\"...\"\\n```\\n\\nWe can then initialize the model:\\n\\n```python\\nfrom langchain_community.chat_models import ChatOpenAI\\n\\nllm = ChatOpenAI()\\n```\\n\\nIf you\\'d prefer not to set an environment variable you can pass the key in directly via the `openai_api_key` named parameter when initiating the OpenAI LLM class:\\n\\n```python\\nfrom langchain_community.chat_models import ChatOpenAI\\n\\nllm = ChatOpenAI(openai_api_key=\"...\")\\n```\\n\\n  </TabItem>\\n  <TabItem value=\"local\" label=\"Local\">\\n\\n[Ollama](https://ollama.ai/) allows you to run open-source large language models, such as Llama 2, locally.\\n\\nFirst, follow [these instructions](https://github.com/jmorganca/ollama) to set up and run a local Ollama instance:\\n\\n* [Download](https://ollama.ai/download)\\n* Fetch a model via `ollama pull llama2`\\n\\nThen, make sure the Ollama server is running. After that, you can do:\\n```python\\nfrom langchain_community.llms import Ollama\\nllm = Ollama(model=\"llama2\")\\n```\\n\\n  </TabItem>\\n</Tabs>\\n\\nOnce you\\'ve installed and initialized the LLM of your choice, we can try using it!\\nLet\\'s ask it what LangSmith is - this is something that wasn\\'t present in the training data so it shouldn\\'t have a very good response.\\n\\n```python\\nllm.invoke(\"how can langsmith help with testing?\")\\n```\\n\\nWe can also guide it\\'s response with a prompt template.\\nPrompt templates are used to convert raw user input to a better input to the LLM.\\n\\n```python\\nfrom langchain.prompts import ChatPromptTemplate\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are world class technical documentation writer.\"),\\n    (\"user\", \"{input}\")\\n])\\n```\\n\\nWe can now combine these into a simple LLM chain:\\n\\n```python\\nchain = prompt | llm \\n```\\n\\nWe can now invoke it and ask the same question. It still won\\'t know the answer, but it should respond in a more proper tone for a technical writer!\\n\\n```python\\nchain.invoke({\"input\": \"how can langsmith help with testing?\"})\\n```\\n\\nThe output of a ChatModel (and therefore, of this chain) is a message. However, it\\'s often much more convenient to work with strings. Let\\'s add a simple output parser to convert the chat message to a string.\\n\\n```python\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\noutput_parser = StrOutputParser()\\n```\\n\\nWe can now add this to the previous chain:\\n\\n```python\\nchain = prompt | llm | output_parser\\n```\\n\\nWe can now invoke it and ask the same question. The answer will now be a string (rather than a ChatMessage).\\n\\n```python\\nchain.invoke({\"input\": \"how can langsmith help with testing?\"})\\n```\\n\\n### Diving Deeper\\n\\nWe\\'ve now successfully set up a basic LLM chain. We only touched on the basics of prompts, models, and output parsers - for a deeper dive into everything mentioned here, see [this section of documentation](/docs/modules/model_io).\\n\\n\\n## Retrieval Chain\\n\\nIn order to properly answer the original question (\"how can langsmith help with testing?\"), we need to provide additional context to the LLM.\\nWe can do this via *retrieval*.\\nRetrieval is useful when you have **too much data** to pass to the LLM directly.\\nYou can then use a retriever to fetch only the most relevant pieces and pass those in.\\n\\nIn this process, we will look up relevant documents from a *Retriever* and then pass them into the prompt.\\nA Retriever can be backed by anything - a SQL table, the internet, etc - but in this instance we will populate a vector store and use that as a retriever. For more information on vectorstores, see [this documentation](/docs/modules/data_connection/vectorstores).\\n\\nFirst, we need to load the data that we want to index:\\n\\n\\n```python\\nfrom langchain_community.document_loaders import WebBaseLoader\\nloader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\\n\\ndocs = loader.load()\\n```\\n\\nNext, we need to index it into a vectorstore. This requires a few components, namely an [embedding model](/docs/modules/data_connection/text_embedding) and a [vectorstore](/docs/modules/data_connection/vectorstores).\\n\\nFor embedding models, we once again provide examples for accessing via OpenAI or via local models.\\n\\n<Tabs>\\n  <TabItem value=\"openai\" label=\"OpenAI\" default>\\n  \\nMake sure you have the openai package installed an the appropriate environment variables set (these are the same as needed for the LLM).\\n\\n```python\\nfrom langchain_community.embeddings import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings()\\n```\\n\\n</TabItem>\\n<TabItem value=\"local\" label=\"Local\">\\n\\nMake sure you have Ollama running (same set up as with the LLM).\\n\\n```python\\nfrom langchain_community.embeddings import OllamaEmbeddings\\n\\nembeddings = OllamaEmbeddings()\\n```\\n  </TabItem>\\n</Tabs>\\n\\nNow, we can use this embedding model to ingest documents into a vectorstore.\\nWe will use a simple local vectorstore, [DocArray InMemorySearch](/docs/integrations/vectorstores/docarray_in_memory), for simplicity\\'s sake.\\n\\nFirst we need to install the required packages for that:\\n\\n```shell\\npip install docarray\\npip install tiktoken\\n```\\n\\nThen we can build our index:\\n\\n```python\\nfrom langchain_community.vectorstores import DocArrayInMemorySearch\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\n\\ntext_splitter = RecursiveCharacterTextSplitter()\\ndocuments = text_splitter.split_documents(docs)\\nvector = DocArrayInMemorySearch.from_documents(documents, embeddings)\\n```\\n\\nNow that we have this data indexed in a vectorstore, we will create a retrieval chain.\\nThis chain will take an incoming question, look up relevant documents, then pass those documents along with the original question into an LLM and ask it to answer the original question.\\n\\nFirst, let\\'s set up the chain that takes a question and the retrieved documents and generates an answer.\\n\\n```python\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\n\\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\\n\\n<context>\\n{context}\\n</context>\\n\\nQuestion: {input}\"\"\")\\n\\ndocument_chain = create_stuff_documents_chain(llm, prompt)\\n```\\n\\nIf we wanted to, we could run this ourselves by passing in documents directly:\\n\\n```python\\nfrom langchain_core.documents import Document\\n\\ndocument_chain.invoke({\\n    \"input\": \"how can langsmith help with testing?\",\\n    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\\n})\\n```\\n\\nHowever, we want the documents to first come from the retriever we just set up.\\nThat way, for a given question we can use the retriever to dynamically select the most relevant documents and pass those in.\\n\\n```python\\nfrom langchain.chains import create_retrieval_chain\\n\\nretriever = vector.as_retriever()\\nretrieval_chain = create_retrieval_chain(retriever, document_chain)\\n```\\n\\nWe can now invoke this chain. This returns a dictionary - the response from the LLM is in the `answer` key\\n\\n```python\\nresponse = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\\nprint(response[\"answer\"])\\n\\n# LangSmith offers several features that can help with testing:...\\n```\\n\\nThis answer should be much more accurate!\\n\\n### Diving Deeper\\n\\nWe\\'ve now successfully set up a basic retrieval chain. We only touched on the basics of retrieval - for a deeper dive into everything mentioned here, see [this section of documentation](/docs/modules/data_connection).\\n\\n## Conversation Retrieval Chain\\n\\nThe chain we\\'ve created so far can only answer single questions. One of the main types of LLM applications that people are building are chat bots. So how do we turn this chain into one that can answer follow up questions?\\n\\nWe can still use the `create_retrieval_chain` function, but we need to change two things:\\n\\n1. The retrieval method should now not just work on the most recent input, but rather should take the whole history into account.\\n2. The final LLM chain should likewise take the whole history into account\\n\\n**Updating Retrieval**\\n\\nIn order to update retrieval, we will create a new chain. This chain will take in the most recent input (`input`) and the conversation history (`chat_history`) and use an LLM to generate a search query.\\n\\n```python\\nfrom langchain.chains import create_history_aware_retriever\\nfrom langchain_core.prompts import MessagesPlaceholder\\n\\n# First we need a prompt that we can pass into an LLM to generate this search query\\n\\nprompt = ChatPromptTemplate.from_messages([\\n    MessagesPlaceholder(variable_name=\"chat_history\"),\\n    (\"user\", \"{input}\"),\\n    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\\n])\\nretriever_chain = create_history_aware_retriever(llm, retriever, prompt)\\n```\\n\\nWe can test this out by passing in an instance where the user is asking a follow up question.\\n\\n```python\\nfrom langchain_core.messages import HumanMessage, AIMessage\\n\\nchat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\\nretriever_chain.invoke({\\n    \"chat_history\": chat_history,\\n    \"input\": \"Tell me how\"\\n})\\n```\\nYou should see that this returns documents about testing in LangSmith. This is because the LLM generated a new query, combining the chat history with the follow up question.\\n\\nNow that we have this new retriever, we can create a new chain to continue the conversation with these retrieved documents in mind.\\n\\n```python\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"Answer the user\\'s questions based on the below context:\\\\n\\\\n{context}\"),\\n    MessagesPlaceholder(variable_name=\"chat_history\"),\\n    (\"user\", \"{input}\"),\\n])\\ndocument_chain = create_stuff_documents_chain(llm, prompt)\\n\\nretrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\\n```\\n\\nWe can now test this out end-to-end:\\n\\n```python\\nchat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\\nretrieval_chain.invoke({\\n    \"chat_history\": chat_history,\\n    \"input\": \"Tell me how\"\\n})\\n```\\nWe can see that this gives a coherent answer - we\\'ve successfully turned our retrieval chain into a chatbot!\\n\\n## Agent\\n\\nWe\\'ve so far create examples of chains - where each step is known ahead of time.\\nThe final thing we will create is an agent - where the LLM decides what steps to take.\\n\\n**NOTE: for this example we will only show how to create an agent using OpenAI models, as local models are not reliable enough yet.**\\n\\nOne of the first things to do when building an agent is to decide what tools it should have access to.\\nFor this example, we will give the agent access two tools:\\n\\n1. The retriever we just created. This will let it easily answer questions about LangSmith\\n2. A search tool. This will let it easily answer questions that require up to date information.\\n\\nFirst, let\\'s set up a tool for the retriever we just created:\\n\\n```python\\nfrom langchain.tools.retriever import create_retriever_tool\\n\\nretriever_tool = create_retriever_tool(\\n    retriever,\\n    \"langsmith_search\",\\n    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\\n)\\n```\\n\\n\\nThe search tool that we will use is [Tavily](/docs/integrations/retrievers/tavily). This will require an API key (they have generous free tier). After creating it on their platform, you need to set it as an environment variable:\\n\\n```shell\\nexport TAVILY_API_KEY=...\\n```\\nIf you do not want to set up an API key, you can skip creating this tool.\\n\\n```python\\nfrom langchain_community.tools.tavily_search import TavilySearchResults\\n\\nsearch = TavilySearchResults()\\n```\\n\\nWe can now create a list of the tools we want to work with:\\n\\n```python\\ntools = [retriever_tool, search]\\n```\\n\\nNow that we have the tools, we can create an agent to use them. We will go over this pretty quickly - for a deeper dive into what exactly is going on, check out the [Agent\\'s Getting Started documentation](/docs/modules/agents)\\n\\nInstall langchain hub first\\n```bash\\npip install langchainhub\\n```\\n\\nNow we can use it to get a predefined prompt\\n\\n```python\\nfrom langchain_community.chat_models import ChatOpenAI\\nfrom langchain import hub\\nfrom langchain.agents import create_openai_functions_agent\\nfrom langchain.agents import AgentExecutor\\n\\n# Get the prompt to use - you can modify this!\\nprompt = hub.pull(\"hwchase17/openai-functions-agent\")\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\\nagent = create_openai_functions_agent(llm, tools, prompt)\\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\\n```\\n\\nWe can now invoke the agent and see how it responds! We can ask it questions about LangSmith:\\n\\n```python\\nagent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})\\n```\\n\\nWe can ask it about the weather:\\n\\n```python\\nagent_executor.invoke({\"input\": \"what is the weather in SF?\"})\\n```\\n\\nWe can have conversations with it:\\n\\n```python\\nchat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\\nagent_executor.invoke({\\n    \"chat_history\": chat_history,\\n    \"input\": \"Tell me how\"\\n})\\n```\\n\\n### Diving Deeper\\n\\nWe\\'ve now successfully set up a basic agent. We only touched on the basics of agents - for a deeper dive into everything mentioned here, see [this section of documentation](/docs/modules/agents).\\n\\n\\n## Serving with LangServe\\n\\nNow that we\\'ve built an application, we need to serve it. That\\'s where LangServe comes in.\\nLangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to use LangChain, but in this guide we\\'ll show how you can deploy your app with LangServe.\\n\\nWhile the first part of this guide was intended to be run in a Jupyter Notebook, we will now move out of that. We will be creating a Python file and then interacting with it from the command line.\\n\\nInstall with:\\n```bash\\npip install \"langserve[all]\"\\n```\\n\\n### Server\\n\\nTo create a server for our application we\\'ll make a `serve.py` file. This will contain our logic for serving our application. It consists of three things:\\n1. The definition of our chain that we just built above\\n2. Our FastAPI app\\n3. A definition of a route from which to serve the chain, which is done with `langserve.add_routes`\\n\\n```python\\n#!/usr/bin/env python\\nfrom typing import List\\n\\nfrom fastapi import FastAPI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain_community.chat_models import ChatOpenAI\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_community.embeddings import OpenAIEmbeddings\\nfrom langchain_community.vectorstores import DocArrayInMemorySearch\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.tools.retriever import create_retriever_tool\\nfrom langchain_community.tools.tavily_search import TavilySearchResults\\nfrom langchain_community.chat_models import ChatOpenAI\\nfrom langchain import hub\\nfrom langchain.agents import create_openai_functions_agent\\nfrom langchain.agents import AgentExecutor\\nfrom langchain.pydantic_v1 import BaseModel, Field\\nfrom langchain_core.messages import BaseMessage\\nfrom langserve import add_routes\\n\\n# 1. Load Retriever\\nloader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\\ndocs = loader.load()\\ntext_splitter = RecursiveCharacterTextSplitter()\\ndocuments = text_splitter.split_documents(docs)\\nembeddings = OpenAIEmbeddings()\\nvector = DocArrayInMemorySearch.from_documents(documents, embeddings)\\nretriever = vector.as_retriever()\\n\\n# 2. Create Tools\\nretriever_tool = create_retriever_tool(\\n    retriever,\\n    \"langsmith_search\",\\n    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\\n)\\nsearch = TavilySearchResults()\\ntools = [retriever_tool, search]\\n\\n\\n# 3. Create Agent\\nprompt = hub.pull(\"hwchase17/openai-functions-agent\")\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\\nagent = create_openai_functions_agent(llm, tools, prompt)\\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\\n\\n\\n# 4. App definition\\napp = FastAPI(\\n  title=\"LangChain Server\",\\n  version=\"1.0\",\\n  description=\"A simple API server using LangChain\\'s Runnable interfaces\",\\n)\\n\\n# 5. Adding chain route\\n\\n# We need to add these input/output schemas because the current AgentExecutor\\n# is lacking in schemas.\\n\\nclass Input(BaseModel):\\n    input: str\\n    chat_history: List[BaseMessage] = Field(\\n        ...,\\n        extra={\"widget\": {\"type\": \"chat\", \"input\": \"location\"}},\\n    )\\n\\n\\nclass Output(BaseModel):\\n    output: str\\n\\nadd_routes(\\n    app,\\n    agent_executor.with_types(input_type=Input, output_type=Output),\\n    path=\"/agent\",\\n)\\n\\nif __name__ == \"__main__\":\\n    import uvicorn\\n\\n    uvicorn.run(app, host=\"localhost\", port=8000)\\n```\\n\\nAnd that\\'s it! If we execute this file:\\n```bash\\npython serve.py\\n```\\nwe should see our chain being served at localhost:8000.\\n\\n### Playground\\n\\nEvery LangServe service comes with a simple built-in UI for configuring and invoking the application with streaming output and visibility into intermediate steps.\\nHead to http://localhost:8000/agent/playground/ to try it out! Pass in the same question as before - \"how can langsmith help with testing?\" - and it should respond same as before.\\n\\n### Client\\n\\nNow let\\'s set up a client for programmatically interacting with our service. We can easily do this with the `[langserve.RemoteRunnable](/docs/langserve#client)`.\\nUsing this, we can interact with the served chain as if it were running client-side.\\n\\n```python\\nfrom langserve import RemoteRunnable\\n\\nremote_chain = RemoteRunnable(\"http://localhost:8000/agent/\")\\nremote_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\\n```\\n\\nTo learn more about the many other features of LangServe [head here](/docs/langserve).\\n\\n## Next steps\\n\\nWe\\'ve touched on how to build an application with LangChain, how to trace it with LangSmith, and how to serve it with LangServe.\\nThere are a lot more features in all three of these than we can cover here.\\nTo continue on your journey, we recommend you read the following (in order):\\n\\n- All of these features are backed by [LangChain Expression Language (LCEL)](/docs/expression_language) - a way to chain these components together. Check out that documentation to better understand how to create custom chains.\\n- [Model IO](/docs/modules/model_io) covers more details of prompts, LLMs, and output parsers.\\n- [Retrieval](/docs/modules/data_connection) covers more details of everything related to retrieval\\n- [Agents](/docs/modules/agents) covers details of everything related to agents\\n- Explore common [end-to-end use cases](/docs/use_cases/qa_structured/sql) and [template applications](/docs/templates)\\n- [Read up on LangSmith](/docs/langsmith/), the platform for debugging, testing, monitoring and more\\n- Learn more about serving your applications with [LangServe](/docs/langserve)\\n', metadata={'source': './langchain/docs/docs/get_started/quickstart.mdx'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cc6c80-d84f-4455-840c-3778228fcde3",
   "metadata": {},
   "source": [
    "### 디렉토리 파일들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c69fc97e-3bbb-489f-8896-323c2e62251c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (0.14.5)\n",
      "Requirement already satisfied: chardet in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (5.2.0)\n",
      "Requirement already satisfied: filetype in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: lxml in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (5.2.2)\n",
      "Requirement already satisfied: nltk in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (3.8.1)\n",
      "Requirement already satisfied: tabulate in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (0.9.0)\n",
      "Requirement already satisfied: requests in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (2.32.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (4.12.3)\n",
      "Requirement already satisfied: emoji in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (2.12.1)\n",
      "Requirement already satisfied: dataclasses-json in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (0.6.6)\n",
      "Requirement already satisfied: python-iso639 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (2024.4.27)\n",
      "Requirement already satisfied: langdetect in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: numpy in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (1.26.4)\n",
      "Requirement already satisfied: rapidfuzz in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (3.9.3)\n",
      "Requirement already satisfied: backoff in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (4.11.0)\n",
      "Requirement already satisfied: unstructured-client in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (0.23.2)\n",
      "Requirement already satisfied: wrapt in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured) (1.16.0)\n",
      "Requirement already satisfied: markdown in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured[md]) (3.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from beautifulsoup4->unstructured) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from dataclasses-json->unstructured) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from langdetect->unstructured) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from nltk->unstructured) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from nltk->unstructured) (4.66.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from requests->unstructured) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from requests->unstructured) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from requests->unstructured) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from requests->unstructured) (2024.2.2)\n",
      "Requirement already satisfied: deepdiff>=6.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured-client->unstructured) (7.0.1)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured-client->unstructured) (0.27.0)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured-client->unstructured) (1.0.6)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: packaging>=23.1 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured-client->unstructured) (23.2)\n",
      "Requirement already satisfied: pypdf>=4.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured-client->unstructured) (4.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured-client->unstructured) (2.9.0.post0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from deepdiff>=6.0->unstructured-client->unstructured) (4.1.0)\n",
      "Requirement already satisfied: anyio in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install unstructured \"unstructured[md]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5732c518-625c-4cfc-b3e2-39669403649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3618e0c6-bc51-49b2-9cce-3e6119b8f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('./langchain/docs/docs/', glob=\"**/*.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2184d19e-67f7-45f3-a74c-f953ab25f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cd95113-d6b5-46e4-afbf-347be7886604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbebfe11-343c-4885-b603-30a0c54e12a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='sidebar_class_name: hidden\\n\\nLangSmith\\n\\nLangSmith helps you trace and evaluate your language model applications and intelligent agents to help you\\nmove from prototype to production.\\n\\nCheck out the interactive walkthrough to get started.\\n\\nFor more information, please refer to the LangSmith documentation.\\n\\nFor tutorials and other end-to-end examples demonstrating ways to integrate LangSmith in your workflow,\\ncheck out the LangSmith Cookbook. Some of the guides therein include:\\n\\nLeveraging user feedback in your JS application (link).\\n\\nBuilding an automated feedback pipeline (link).\\n\\nHow to evaluate and audit your RAG workflows (link).\\n\\nHow to fine-tune an LLM on real usage data (link).\\n\\nHow to use the LangChain Hub to version your prompts (link)', metadata={'source': 'langchain/docs/docs/langsmith/index.md'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66751269-0880-4034-a9c8-555e432ad1e7",
   "metadata": {},
   "source": [
    "### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5712cf37-2e9a-4786-99c8-724f8fe8c8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (4.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4c529c6-2533-4ee4-a446-c768a0c13c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./langchain/libs/community/tests/examples/layout-parser-paper.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7e483d6-3b68-477b-ad8c-585582c85aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cbe93bd-8f63-49c0-be20-ee41653811fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='2 Z. Shen et al.\\n37], layout detection [ 38,22], table detection [ 26], and scene text detection [ 4].\\nA generalized learning-based framework dramatically reduces the need for the\\nmanual speciﬁcation of complicated rules, which is the status quo with traditional\\nmethods. DL has the potential to transform DIA pipelines and beneﬁt a broad\\nspectrum of large-scale document digitization projects.\\nHowever, there are several practical diﬃculties for taking advantages of re-\\ncent advances in DL-based methods: 1) DL models are notoriously convoluted\\nfor reuse and extension. Existing models are developed using distinct frame-\\nworks like TensorFlow [ 1] or PyTorch [ 24], and the high-level parameters can\\nbe obfuscated by implementation details [ 8]. It can be a time-consuming and\\nfrustrating experience to debug, reproduce, and adapt existing models for DIA,\\nand many researchers who would beneﬁt the most from using these methods lack\\nthe technical background to implement them from scratch. 2) Document images\\ncontain diverse and disparate patterns across domains, and customized training\\nis often required to achieve a desirable detection accuracy. Currently there is no\\nfull-ﬂedged infrastructure for easily curating the target document image datasets\\nand ﬁne-tuning or re-training the models. 3) DIA usually requires a sequence of\\nmodels and other processing to obtain the ﬁnal outputs. Often research teams use\\nDL models and then perform further document analyses in separate processes,\\nand these pipelines are not documented in any central location (and often not\\ndocumented at all). This makes it diﬃcult for research teams to learn about how\\nfull pipelines are implemented and leads them to invest signiﬁcant resources in\\nreinventing the DIA wheel .\\nLayoutParser provides a uniﬁed toolkit to support DL-based document image\\nanalysis and processing. To address the aforementioned challenges, LayoutParser\\nis built with the following components:\\n1.An oﬀ-the-shelf toolkit for applying DL models for layout detection, character\\nrecognition, and other DIA tasks (Section 3)\\n2.A rich repository of pre-trained neural network models (Model Zoo) that\\nunderlies the oﬀ-the-shelf usage\\n3.Comprehensive tools for eﬃcient document image data annotation and model\\ntuning to support diﬀerent levels of customization\\n4.A DL model hub and community platform for the easy sharing, distribu-\\ntion, and discussion of DIA models and pipelines, to promote reusability,\\nreproducibility, and extensibility (Section 4)\\nThe library implements simple and intuitive Python APIs without sacriﬁcing\\ngeneralizability and versatility, and can be easily installed via pip. Its convenient\\nfunctions for handling document image data can be seamlessly integrated with\\nexisting DIA pipelines. With detailed documentations and carefully curated\\ntutorials, we hope this tool will beneﬁt a variety of end-users, and will lead to\\nadvances in applications in both industry and academic research.\\nLayoutParser is well aligned with recent eﬀorts for improving DL model\\nreusability in other disciplines like natural language processing [ 8,34] and com-\\nputer vision [ 35], but with a focus on unique challenges in DIA. We show\\nLayoutParser can be applied in sophisticated and large-scale digitization projects', metadata={'source': './langchain/libs/community/tests/examples/layout-parser-paper.pdf', 'page': 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc4b2bc-3b6a-4b22-8dc1-d705d2e6053a",
   "metadata": {},
   "source": [
    "## 검색하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0288168f-4b4c-44c2-a868-8cc99037aa1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: faiss-cpu in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (1.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: numpy in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14c7754a-a721-4aab-b089-a7392b4a1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "142586c9-ab06-4205-8eef-edfc317e0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"./data/소개팅_나무위키.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "175168bb-20a1-47de-9de5-5b6becae668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3374956b-bc3c-464e-9f90-4f87fecfc6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33ee3b55-5be9-421f-a44b-344f0666588b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='24. 1. 7. 오후  1:32 소개팅  - 나무위키\\nhttps://namu.wiki/w/ 소개팅 4/18\\uf3d0\\n\\uf3d01. 아무리  퀸카라  하더라도  여중 , 여고 , 여대에  직장마저  여초  직장을  갖게  된다면  현실적으로  주변에서  자연스럽게  남자를\\n찾기는  어렵다 .[4]\\n2. 어린 여성의  경우에는 , 대학  졸업을  기점으로  촌티를  벗고  외모의  급격한  향상이  이루어지는  경우가  많다 . 나이가  든  이후\\n에도 수험생활이  길거나 , 학위  취득에  전념하다보니  외모관리에  소홀해지는  경우도  흔하다 . 그런데 , 소개팅  시장에  주로\\n나오는  여성들은  주로  이런  부류의  사람들이다 . 따라서  처음  보이는  인상만으로  실제로  그  여성이  계속  매력이  없을  것이\\n라고 생각할  수  없다 .\\n3. 소개팅  외에  이성을  만날  여건  자체가  안  되는  경우가  많다 . 직장인의  경우  주변에  마음에  들지  않는  이성  밖에  없으며  사\\n내 연애를 부담스러워  하는  경우에는  주변에  사실상  연애를  할  사람이  없는  셈이다 . 이  경우  사실상  소개팅  외에는  이성을\\n만날 기회를  얻기  어렵다 .[5]\\n4. 동호회  등  외부  사교활동으로  만날  수  있지  않느냐는  의견이  있을  수  있지만 , 외부  활동을  별로  좋아하지  않는  성격인  경우\\n굳이 시간을  쪼개어  연애를  하기  위해  동아리나  동호회의  부수적  활동까지  하기를  원치는  않는  경우도  적지  않다 . 또한 , 여\\n성의 경우  남성이  많은  동호회에  가입할  시  여왕벌 취급을  받는  것에  부담감을  느끼거나  혐오감을  느끼기도  한다 . 반면에\\n남성의  경우  여초  동호회에  가입하는  것에  대해  여자를  밝히는  사람인  것처럼  오해를  받는  데  상당한  부담을  느끼기도  한\\n다.\\n5. 이상의  점  때문에  동아리나  동호회  등  사교활동은  하기  원치  않고 , 맞선  등  대놓고  결혼을  전제하는  만남은  부담스러운  경\\n우 효율  측면에서  소개팅을  택할  수도  있다 .\\n6. 연애 또한  사람과의  관계로서  일종의  사회생활이라고  한다면 , 스스로  눈높이를  맞추어가는  과정은  불가피하게  발생한다 .\\n다른 인생의  관문인  대입 , 취직의  경우와  비슷하다고  보면  된다 . 어렸을  때에는  대부분의  학생들이  조금만  공부하면  SKY,\\n최악의  경우에도  인서울 대학에  간다고  생각하지만 , 학년이  오르면  오를수록  원서를  내고자  하는  대학교  기준이  현실적이\\n되어가는  자신을  발견하듯이  연애  경험이  적고 , 현실  감각이  적을수록  자신의  연애  대상에  대한  기준은  추상적이거나  비현\\n실적일  가능성이  높다 . 그런  상황에서는  현실의  이성  누구를  만나더라도  눈에  차지  않을  수밖에  없다 .[6]\\n7. 객관적으로  보기에  특별한  하자가  없는  남녀  중에도 , 미팅이나  소개팅  자체에서  오는  긴장감과  재미를  즐기기  때문에  자주\\n소개팅에  나오는  남녀도  많다 .\\n3.2. 주의 사항\\n소개팅  시장에  공급이  많아지는  시점도  있다 . 남자든  여자든  처한  환경이  크게  변할  때  기존  연인과  헤어져  \" 솔로 \" 상태가  되는\\n경우가  많기  때문이다 . 대학생  때  오래  사귄  연인도  보통  대학  졸업  후  취업  직후 ~2 년차에  많이들  헤어진다 . 기관  연수원 & 기업\\n연수원에서부터  커플  브레이킹이  일어나는  경우가  많다 . 여성은  27 정도 , 남성은  29 정도에  소개팅  시장으로  많은  진입이  있기\\n에 이 시기  소개팅을  구하기가  쉽다 . 또한  이  시기에는  좋은  매물 (?) 이  많이  나온다고  평가  받는다 .\\n그리고  사회  생활에  익숙해지는  3~4 년차에  연애할  만한  시간적  정신적  여유가  생기는  것이  보통 . 아무래도  이  시기에  연애를\\n시작하는  경우가  많고 , 연애에  관한  남녀  고민이  집중되기도  한다 . 이  시점에  만나는  연인은  결혼할  확률도  높다 . 즉  위  타이밍\\n[7]을 놓치면  소위  말하는  \" 멋진 \" 상대는  소개팅에서  희소해진다 .\\n3.3. 소개팅이  성공할  확률은  매우  낮다\\n남자든  여자든  소개팅에  몇번  실패하면  자신의  조건이  남들에  비해  떨어지거나  부족한게  아닐까  자책하는  경우가  종종  있다 . 특\\n히 이성  교제  경험이  없거나  적은  경우일수록  더더욱 . 하지만  소개팅의  타율은  굉장히  낮기  때문에  실패  횟수가  많다고  해서  크\\n게 자책할  필요는  없다 .\\n일단 내  마음에  드는  괜찮은  상대가  나타날  확률이  극히  적다 . 보통  남녀가  결혼정보회사  회원에게  설문조사한  결과  애프터[8]\\n성공률은  10 명  만날  때마다  4 명  정도였다 . 그리고  에프터에  우선  성공한  4 명  중  3 명도  애프터  시기  몇번  사이에  돌려서  거절\\n당하므로 , 실제로  10 명  소개팅을  받으면  보통  1 명  정도가  사귀는  관계로  발전한다 . 실제  유의할  부분은  결정사회원의  경우는\\n소개팅보다는  맞선에 가까운  만남을  가진다는  것이다 . 즉  필터링할  것을  다  하고  양  당사자  모두  \\' 그럴  듯하게  맞을  만하다 \\' 고\\n판단하거나  매칭  매니저의  상담을  받고  신중하게  만남을  시작해도 , 1 회의  만남에  그치는  경우가  과반이라는  것 . 보통  이성  교제', metadata={'source': './data/소개팅_나무위키.pdf', 'page': 3})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ea94c92-d75f-4560-a1a5-adb61cbb50f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 107, which is longer than the specified 100\n",
      "Created a chunk of size 118, which is longer than the specified 100\n",
      "Created a chunk of size 113, which is longer than the specified 100\n",
      "Created a chunk of size 117, which is longer than the specified 100\n",
      "Created a chunk of size 106, which is longer than the specified 100\n",
      "Created a chunk of size 118, which is longer than the specified 100\n",
      "Created a chunk of size 102, which is longer than the specified 100\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=100, chunk_overlap=0, length_function=len)\n",
    "texts = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82dd53d5-12fd-449d-be42-2bc0ef460321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "581"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbed9893-8a04-470b-8f6e-e7e6263fc95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='24. 1. 7. 오후  1:32 소개팅  - 나무위키\\nhttps://namu.wiki/w/ 소개팅 1/18\\uf3d0소개팅 \\n최근 수정  시각 : 2023-12-22 13:06:41', metadata={'source': './data/소개팅_나무위키.pdf', 'page': 0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e2b768a-8724-478b-b161-3465a9735dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ffaeb17-4019-4c71-9ede-5e7a224a7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "beee1712-9916-4ba6-878b-df73a03d3682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/miniconda3/envs/fc-ai-signature/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "rel_docs = retriever.get_relevant_documents(\"주선자의 역할\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "353ea9e8-ef66-4885-9dfa-a1bf47a9326c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rel_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b70975b8-7005-4383-98bc-23f2fae30cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='2. 주선자와  그  역할\\n과거에는  주선자가  대상  남녀가  만날  장소까지  주선해  주고  잠시  동석하여  각자  소개  및  대화를  잠깐  이끌어  주었다가  자리를', metadata={'source': './data/소개팅_나무위키.pdf', 'page': 1})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2934fba9-22a6-494c-9735-4bf8dff104cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='2. 주선자와  그  역할\\n과거에는  주선자가  대상  남녀가  만날  장소까지  주선해  주고  잠시  동석하여  각자  소개  및  대화를  잠깐  이끌어  주었다가  자리를', metadata={'source': './data/소개팅_나무위키.pdf', 'page': 1}),\n",
       " Document(page_content='주선자의  역할이  줄어들다  보니 , 아예  주선자조차  남녀를  둘  다  알지  못하는  한다리  건너식  소개팅  양상도  늘어났다 . 예컨대  A', metadata={'source': './data/소개팅_나무위키.pdf', 'page': 1}),\n",
       " Document(page_content='는 주선자  역할인  a 는  A 와  b 만  잘  알  뿐  정작  소개팅  상대녀인  B 에  대한  정보는  한다리  건너서만  아는  식 .', metadata={'source': './data/소개팅_나무위키.pdf', 'page': 1})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a146e30-03b3-43fe-b31b-874a99cea73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rel_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30dda61b-c517-488d-bec3-81410de78c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주선자의  역할이  줄어들다  보니 , 아예  주선자조차  남녀를  둘  다  알지  못하는  한다리  건너식  소개팅  양상도  늘어났다 . 예컨대  A\n"
     ]
    }
   ],
   "source": [
    "print(rel_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f5479c-c87a-4278-b0f4-6bcf58c10aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
